{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- if SGAMATO, extend sleep time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import random\n",
    "import threading, Queue\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []\n",
    "\n",
    "def build_headers():\n",
    "    \n",
    "    header_list = []\n",
    "    for i in range(1000):\n",
    "        header_list.append(ua.random)\n",
    "\n",
    "    headers = [{'User-Agent' : i} for i in header_list]\n",
    "\n",
    "build_headers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "scrape proxies from free-proxy-list.net/anonymous-proxy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxies found: 398\n"
     ]
    }
   ],
   "source": [
    "def retrieve_proxies():\n",
    "    \n",
    "    proxies = set()\n",
    "\n",
    "    urls = ['https://free-proxy-list.net/anonymous-proxy.html', 'https://www.us-proxy.org/', \\\n",
    "            'https://www.sslproxies.org/', 'https://www.socks-proxy.net/']\n",
    "    \n",
    "    for url in urls:\n",
    "        res = requests.get(url, headers={'User-Agent' : ua.random})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print \"CONNECTION ERROR ON RETRIEVING PROXIES\"\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        elems = soup.findAll(\"table\", { \"id\" : \"proxylisttable\" })\n",
    "\n",
    "        for i in elems:\n",
    "            cells = i.find_all('td')\n",
    "            for cell in cells:\n",
    "                if cell.string != None and re.match('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', cell.string) != None: \n",
    "                    proxies.add(cell.string)\n",
    "                \n",
    "    return list(proxies)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        proxies = retrieve_proxies()\n",
    "        if len(proxies) > 0: break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "print \"proxies found: \" + str(len(proxies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step takes a list of asins as input and collects their review pages urls\n",
    "* I should check here if the product actually exists and mess with the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nproducts = ['B01H2E0J5M']\\n\\nwhile True:\\n    try:\\n        pages_hrefs = get_review_pages_hrefs(products)\\n        if len(pages_hrefs)>0: \\n            break\\n    except Exception as error:\\n        print('caught this error: ' + repr(error))\\n        time.sleep(0.5)\\n\\nprint pages_hrefs\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_review_pages_hrefs(products):\n",
    "    \n",
    "    pages_hrefs = set()\n",
    "    \n",
    "    for asin in products:\n",
    "        \n",
    "        url = 'https://www.amazon.com/gp/product/' + asin\n",
    "        \n",
    "        res = requests.get(url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                           headers={'User-Agent' : ua.random})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print \"CONNECTION PROBLEM ON GETTING REVIEW PAGES HREFS\"\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        \n",
    "        if soup.title.text == \"Robot Check\": \n",
    "            print \"SGAMATO\"\n",
    "            continue\n",
    "\n",
    "        elems = soup.find(\"div\", { \"id\" : \"reviews-medley-footer\" })\n",
    "        elems2 = elems.find(\"a\", { \"class\" : \"a-link-emphasis\" })\n",
    "        \n",
    "        pages_hrefs.add(elems2.get(\"href\"))\n",
    "        \n",
    "        page_file = open(\"./pages/\" + asin + '.html', 'w+')\n",
    "        page_file.write(res.content)\n",
    "    \n",
    "    return pages_hrefs\n",
    "\"\"\"\n",
    "products = ['B01H2E0J5M']\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        pages_hrefs = get_review_pages_hrefs(products)\n",
    "        if len(pages_hrefs)>0: \n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print pages_hrefs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwhile True:\\n    try:\\n        max_page = find_max_pages(list(pages_hrefs)[0])\\n        if max_page>0: \\n            break\\n    except Exception as error:\\n        print('caught this error: ' + repr(error))\\n        time.sleep(0.5)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_pages(href):\n",
    "    hr_url = 'https://www.amazon.com/' + href\n",
    "    res = requests.get(hr_url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                       headers={'User-Agent' : ua.random})\n",
    "\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    if soup.title.text == \"Robot Check\": \n",
    "        print \"SGAMATO\"\n",
    "        \n",
    "    pag_ul = soup.find(\"ul\", {\"class\":\"a-pagination\"})\n",
    "\n",
    "    max_page = 0\n",
    "\n",
    "    for li in pag_ul.find_all(\"li\", {\"class\":\"page-button\"}):\n",
    "        max_page = li.a.text\n",
    "\n",
    "    return max_page\n",
    "\"\"\"\n",
    "while True:\n",
    "    try:\n",
    "        max_page = find_max_pages(list(pages_hrefs)[0])\n",
    "        if max_page>0: \n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print max_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now craft the url!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def craft_urls(base_url, max_pages):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    url_parts = base_url.strip().split(\"/\")\n",
    "    params = url_parts[-1].split('&')\n",
    "\n",
    "    for page_num in range(1, int(max_pages)):\n",
    "        chosen_params = ['ref=cm_cr_dp_d_show_all_btm_'+str(page_num)+'?ie=UTF8', 'pageNumber='+str(page_num)]\n",
    "\n",
    "        final_url = \"https://www.amazon.com/\"\n",
    "\n",
    "        for i, item in enumerate(url_parts):\n",
    "            if i < (len(url_parts)-1) and i>0: final_url += item + '/'\n",
    "\n",
    "        for param in chosen_params:\n",
    "            final_url += param + '&'\n",
    "\n",
    "        final_url = final_url[:-1]\n",
    "        \n",
    "        res.append(final_url)\n",
    "\n",
    "    return res\n",
    "    \n",
    "#final_urls_list = craft_urls(list(pages_hrefs)[0], max_page)\n",
    "#pprint(final_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_page(url, header, proxy, fails, results):\n",
    "    res = \"\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=20, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                           headers={'User-Agent' : ua.random})\n",
    "    except Exception as error:\n",
    "        print \"connection timed out\"\n",
    "        fails.put(url)\n",
    "    \n",
    "    if res == \"\":\n",
    "        print \"Something failed\"\n",
    "    elif res != \"\" and res.status_code != 200:\n",
    "        fails.put(url)\n",
    "    elif BeautifulSoup(res.content, 'html.parser').title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        fails.put(url)\n",
    "    else:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        review_boxes = soup.find_all(\"div\", {\"class\":\"review\"})\n",
    "\n",
    "        for box in review_boxes:\n",
    "            review = {}\n",
    "            review['title'] = box.find(\"a\", {\"class\":\"review-title\"}).text\n",
    "            review['text'] = box.find(\"span\", {\"class\":\"review-text\"}).text\n",
    "            review['date'] = box.find(\"span\", {\"class\":\"review-date\"}).text\n",
    "            review['rating'] = box.find(\"i\", {\"class\":\"review-rating\"}).span.text\n",
    "            review['author'] = box.find(\"a\", {\"class\":\"author\"}).text\n",
    "            review['author_id'] = box.find(\"a\", {\"class\":\"author\"}).get('href').split('/')[4]\n",
    "            results.put(review)\n",
    "\n",
    "def explore_pages(final_urls_list, max_page):\n",
    "    pages = []\n",
    "    threads = []\n",
    "    results = Queue.Queue()\n",
    "    fails = Queue.Queue()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, int(max_page)-1):\n",
    "\n",
    "        if i%10==0: \n",
    "            time.sleep(10)\n",
    "            print i,\n",
    "\n",
    "        f_url = final_urls_list[i]\n",
    "        proxy = proxies[i%len(proxies)]\n",
    "        t = threading.Thread(target=retrieve_page, args=(f_url, headers, proxy, fails, results))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        \n",
    "    return results, fails\n",
    "  \n",
    "#results, fails = explore_pages(final_urls_list, int(max_page))\n",
    "#print(\"elapsed time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nli = list(results.queue)\\nprint len(li)\\n\\nlif = list(fails.queue)\\nprint len(lif)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "li = list(results.queue)\n",
    "print len(li)\n",
    "\n",
    "lif = list(fails.queue)\n",
    "print len(lif)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresults_file = open(\"./results/\"+products[0]+\".json\", \\'w+\\')\\nexamined_file = open(\"examined_asins.txt\", \\'a\\')\\nfailed_file = open(\"failed_urls.txt\", \\'a\\')\\n\\nexamined_file.write(products[0])\\n\\nfor i in li:\\n    json.dump(i, results_file)\\n    results_file.write(\\'\\n\\')\\n    \\nfor i in lif:\\n    failed_file.write(i + \\'\\n\\')\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "results_file = open(\"./results/\"+products[0]+\".json\", 'w+')\n",
    "examined_file = open(\"examined_asins.txt\", 'a')\n",
    "failed_file = open(\"failed_urls.txt\", 'a')\n",
    "\n",
    "examined_file.write(products[0])\n",
    "\n",
    "for i in li:\n",
    "    json.dump(i, results_file)\n",
    "    results_file.write('\\n')\n",
    "    \n",
    "for i in lif:\n",
    "    failed_file.write(i + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_pages_questions = 0\\nwhile True:\\n    try: \\n        max_pages_questions = get_max_pages_questions()\\n        if max_pages_questions > 0:\\n            print \"question pages: \" + max_pages_questions\\n            break\\n    except Exception as error:\\n        print(\\'caught this error: \\' + repr(error))\\n        time.sleep(0.5)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_pages_questions():\n",
    "    url = \"https://www.amazon.com/ask/questions/asin/\" + 'B018IZ0VLQ'\n",
    "    res = requests.get(url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                       headers={'User-Agent' : ua.random})\n",
    "    \n",
    "    max_pages_questions = 0\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print \"CONNECTION ERROR ON RETRIEVING MAX NUMBER OF QUESTIONS\"\n",
    "    \n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    if soup.title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        \n",
    "    for item in soup.find(\"ul\", {\"class\":\"a-pagination\"}).find_all(\"li\", {\"class\":\"a-normal\"}):\n",
    "        max_pages_questions = item.text\n",
    "        \n",
    "    return max_pages_questions\n",
    "\"\"\"\n",
    "max_pages_questions = 0\n",
    "while True:\n",
    "    try: \n",
    "        max_pages_questions = get_max_pages_questions()\n",
    "        if max_pages_questions > 0:\n",
    "            print \"question pages: \" + max_pages_questions\n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_questions(url, header, proxy, fails, results):\n",
    "    res = \"\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=20, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                           headers={'User-Agent' : ua.random})\n",
    "    except Exception as error:\n",
    "        print \"connection timed out\"\n",
    "        fails.put(url)\n",
    "    \n",
    "    if res == \"\":\n",
    "        print \"Something failed\"\n",
    "    elif res != \"\" and res.status_code != 200:\n",
    "        print \"response status: \" + str(res.status_code)\n",
    "        fails.put(url)\n",
    "    elif BeautifulSoup(res.content, 'html.parser').title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        fails.put(url)\n",
    "    else:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        question_boxes = soup.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})\n",
    "\n",
    "        for j, box in enumerate(question_boxes):\n",
    "            q_a_dict = {}\n",
    "            \n",
    "            if j==0: continue\n",
    "            for k, question in enumerate(box.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})):\n",
    "                if k==0: \n",
    "                    #print \"box: \" + str(j) + \" question: \" + question.a.text.strip()\n",
    "                    q_a_dict['question'] = question.a.text.strip()\n",
    "            for k, answer in enumerate(box.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})):\n",
    "                if k!=0: \n",
    "                    ranswer = \"\"\n",
    "                    if answer.find(\"span\", {\"class\":\"askLongText\"}):\n",
    "                        ranswer = answer.find(\"span\", {\"class\":\"askLongText\"}).text\n",
    "                        ranswer = ranswer.strip()[:-8]\n",
    "                    else: \n",
    "                        ranswer = answer.span.text\n",
    "                    q_a_dict['answer'] = ranswer.strip()\n",
    "                    #print \"box: \" + str(j) + \" answer: \" + ranswer\n",
    "            if 'answer' in q_a_dict and 'question' in q_a_dict: results.put(q_a_dict)\n",
    "\n",
    "def explore_questions(product, max_pages_questions):\n",
    "    pages = []\n",
    "    threads = []\n",
    "    results = Queue.Queue()\n",
    "    fails = Queue.Queue()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(1, int(max_pages_questions)):\n",
    "        \n",
    "        if i%10==0: \n",
    "            time.sleep(10)\n",
    "            print i,\n",
    "\n",
    "        f_url = \"https://www.amazon.com/ask/questions/asin/\" + product + '/' + str(i)\n",
    "        proxy = proxies[i%len(proxies)]\n",
    "        t = threading.Thread(target=retrieve_questions, args=(f_url, headers, proxy, fails, results))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        \n",
    "    return results, fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start(products):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for product in products:\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                proxies = retrieve_proxies()\n",
    "                if len(proxies) > 0: break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        print \"proxies found: \" + str(len(proxies))\n",
    "        \n",
    "        print \"examining product: \" + product\n",
    "        \n",
    "        pages_hrefs = []\n",
    "        \n",
    "        asins_with_problems = open(\"asins_with_problems.txt\", 'a')\n",
    "        \n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            try:\n",
    "                pages_hrefs = get_review_pages_hrefs([product])\n",
    "                if len(pages_hrefs)>0:\n",
    "                    break\n",
    "                else: print \"pages_hrefs not found\"\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "            if count%10==0: break\n",
    "        if count==10: \n",
    "            asins_with_problems.write(product + '\\n')\n",
    "            continue\n",
    "\n",
    "        print pages_hrefs\n",
    "\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            try:\n",
    "                max_page = find_max_pages(list(pages_hrefs)[0])\n",
    "                if max_page>0: \n",
    "                    break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "            if count%10==0: break\n",
    "        if count==10: \n",
    "            asins_with_problems.write(product + '\\n')\n",
    "            continue\n",
    "\n",
    "        print max_page\n",
    "\n",
    "        final_urls_list = craft_urls(list(pages_hrefs)[0], max_page)\n",
    "        \n",
    "        results, fails = explore_pages(final_urls_list, int(max_page))\n",
    "        \n",
    "        li = list(results.queue)\n",
    "        print \"\\n results found: \" + str(len(li))\n",
    "\n",
    "        lif = list(fails.queue)\n",
    "        print \"results failed: \" + str(len(lif))\n",
    "        \n",
    "        max_pages_questions = 0\n",
    "        while True:\n",
    "            try: \n",
    "                max_pages_questions = get_max_pages_questions()\n",
    "                if max_pages_questions > 0:\n",
    "                    print \"question pages: \" + max_pages_questions\n",
    "                    break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "                \n",
    "        results_questions, fails_questions = explore_questions(product, max_pages_questions)\n",
    "        \n",
    "        lir = list(results_questions.queue)\n",
    "        print \"\\n results found: \" + str(len(lir))\n",
    "        lifr = list(fails_questions.queue)\n",
    "        print \"results failed: \" + str(len(lifr))\n",
    "        \n",
    "        results_file = open(\"./results/\" + product + \".json\", 'w+')\n",
    "        questions_file = open(\"./questions/\" + product + \".json\", 'w+')\n",
    "        examined_file = open(\"examined_asins.txt\", 'a')\n",
    "        failed_file = open(\"failed_urls.txt\", 'a')\n",
    "        failed_questions_file = open(\"failed_questions_urls.txt\", 'a')\n",
    "\n",
    "        examined_file.write(product + '\\n')\n",
    "\n",
    "        for i in li:\n",
    "            json.dump(i, results_file)\n",
    "            results_file.write('\\n')\n",
    "\n",
    "        for i in lir:\n",
    "            json.dump(i, questions_file)\n",
    "            questions_file.write('\\n')\n",
    "            \n",
    "        for i in lif:\n",
    "            failed_file.write(i + '\\n')\n",
    "            \n",
    "        for i in lifr:\n",
    "            failed_questions_file.write(i + '\\n')\n",
    "    \n",
    "    print \"elapsed time: \" + str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 examined_products\n",
      "138 asins with problems\n",
      "101 products to examine\n",
      "proxies found: 398\n",
      "examining product: B00NJ9HVWO\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 398\n",
      "examining product: B01HP1PJI4\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 398\n",
      "examining product: B074G5G7CF\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 398\n",
      "examining product: B01AYCLXJA\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 398\n",
      "examining product: B00DHUCT5O\n",
      "set([u'/HTC-Unlocked-Android-Smartphone-Beats/product-reviews/B00DHUCT5O/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "2\n",
      "0 SGAMATO\n",
      "\n",
      " results found: 0\n",
      "results failed: 1\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "60 70 80 90 \n",
      " results found: 11\n",
      "results failed: 9\n",
      "proxies found: 398\n",
      "examining product: B017D554UK\n",
      "set([u'/HTC-One-A9-Unlocked-Smartphone/product-reviews/B017D554UK/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "6\n",
      "0 \n",
      " results found: 50\n",
      "results failed: 0\n",
      "question pages: 100\n",
      "10 20 30 40 50 60 70 80 90 \n",
      " results found: 131\n",
      "results failed: 0\n",
      "proxies found: 398\n",
      "examining product: B00R25VZGO\n",
      "set([u'/HTC-610-Unlocked-Quad-Core-Smartphone/product-reviews/B00R25VZGO/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "23\n",
      "0 10 20 \n",
      " results found: 220\n",
      "results failed: 0\n",
      "question pages: 100\n",
      "10 20 30 40 50 60 70 80 90 \n",
      " results found: 169\n",
      "results failed: 0\n",
      "proxies found: 398\n",
      "examining product: B00LOXAHIM\n",
      "set([u'/HTC-M8-Unlocked-Quad-Core-Smartphone/product-reviews/B00LOXAHIM/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "11\n",
      "0 SGAMATO\n",
      "\n",
      " results found: 90\n",
      "results failed: 1\n",
      "question pages: 100\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "10 SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 40 SGAMATO\n",
      "50 SGAMATO\n",
      "SGAMATO\n",
      "60 SGAMATO\n",
      "70 SGAMATO\n",
      "80 SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 90\n",
      "results failed: 14\n",
      "proxies found: 398\n",
      "examining product: B005LSWSSS\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/HTC-Thunderbolt-4G-Verizon-8MP/product-reviews/B005LSWSSS/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "6\n",
      "0 SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "\n",
      " results found: 10\n",
      "results failed: 4\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      "30 40 50 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "60 70 80 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 26\n",
      "results failed: 16\n",
      "proxies found: 402\n",
      "examining product: B017D5F0TU\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 402\n",
      "examining product: B0158TUK3Y\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 402\n",
      "examining product: B01LQV73J2\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/HTC-Desire-Prepaid-Carrier-Verizon/product-reviews/B01LQV73J2/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "4\n",
      "0 SGAMATO\n",
      "\n",
      " results found: 20\n",
      "results failed: 1\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "10 20 SGAMATO\n",
      "30 SGAMATO\n",
      "SGAMATO\n",
      "40 50 SGAMATO\n",
      "SGAMATO\n",
      "60 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "70 SGAMATO\n",
      "SGAMATO\n",
      "80 SGAMATO\n",
      "90 SGAMATO\n",
      "\n",
      " results found: 30\n",
      "results failed: 13\n",
      "proxies found: 402\n",
      "examining product: B01JH5W04M\n",
      "set([u'/HTC-M9-Unlocked-Octa-Core-Smartphone/product-reviews/B01JH5W04M/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "9\n",
      "0 SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "\n",
      " results found: 10\n",
      "results failed: 7\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "question pages: 100\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "10 SGAMATO\n",
      "20 30 SGAMATOSGAMATO\n",
      "SGAMATO\n",
      "\n",
      "40 50 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "60 SGAMATO\n",
      "70 SGAMATO\n",
      "SGAMATO\n",
      "80 SGAMATO\n",
      "SGAMATO\n",
      "90 SGAMATO\n",
      "\n",
      " results found: 145\n",
      "results failed: 15\n",
      "proxies found: 402\n",
      "examining product: B00NJY8GVE\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/HTC-M8-Unlocked-Android-Smartphone/product-reviews/B00NJY8GVE/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "11\n",
      "0 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "\n",
      " results found: 30\n",
      "results failed: 7\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "40 SGAMATO\n",
      "50 60 SGAMATO\n",
      "70 SGAMATO\n",
      "80 SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 77\n",
      "results failed: 18\n",
      "proxies found: 402\n",
      "examining product: B00WVQM3ME\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 402\n",
      "examining product: B004JK26HC\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 402\n",
      "examining product: B0722X3M9F\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 402\n",
      "examining product: B00QHYI488\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/HTC-Desire-Unlocked-Version-Camera/product-reviews/B00QHYI488/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "26\n",
      "0 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "10 SGAMATO\n",
      " SGAMATO\n",
      " SGAMATOSGAMATO\n",
      "\n",
      "SGAMATOSGAMATO\n",
      "\n",
      " SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 SGAMATO\n",
      " SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 30\n",
      "results failed: 22\n",
      "question pages: 100\n",
      "10 20 SGAMATO\n",
      "30 SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "60 SGAMATO\n",
      "70 SGAMATO\n",
      "SGAMATO\n",
      "80 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "90 SGAMATO SGAMATO\n",
      "\n",
      "\n",
      " results found: 388\n",
      "results failed: 13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c32f6fe01233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" products to examine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-407f523b3da2>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(products)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_proxies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-bcb0d52be4ff>\u001b[0m in \u001b[0;36mretrieve_proxies\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'User-Agent'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \"\"\"\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/urllib3/response.pyc\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rana/Envs/amazon/local/lib/python2.7/site-packages/urllib3/response.pyc\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     self.__class__)\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "asins_to_examine = open(\"asins_to_examine.txt\", 'r')\n",
    "examined_asins = open(\"examined_asins.txt\", 'r')\n",
    "asins_with_problems = open(\"asins_with_problems.txt\", 'r')\n",
    "\n",
    "examined_asins_set = set()\n",
    "for line in examined_asins:\n",
    "    examined_asins_set.add(line.strip())\n",
    "    \n",
    "print str(len(examined_asins_set)) + \" examined_products\"\n",
    "\n",
    "asins_with_problems_set = set()\n",
    "for line in asins_with_problems:\n",
    "    asins_with_problems_set.add(line.strip())\n",
    "    \n",
    "print str(len(asins_with_problems_set)) + \" asins with problems\"\n",
    "\n",
    "products = []\n",
    "for line in asins_to_examine:\n",
    "    if line.strip() not in examined_asins_set and line.strip() not in asins_with_problems_set: \n",
    "        products.append(line.strip())\n",
    "    \n",
    "print str(len(products)) + \" products to examine\"\n",
    "start(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "amazon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
