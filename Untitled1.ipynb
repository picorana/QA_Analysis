{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- if SGAMATO, extend sleep time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import random\n",
    "import threading, Queue\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []\n",
    "\n",
    "def build_headers():\n",
    "    \n",
    "    header_list = []\n",
    "    for i in range(1000):\n",
    "        header_list.append(ua.random)\n",
    "\n",
    "    headers = [{'User-Agent' : i} for i in header_list]\n",
    "\n",
    "build_headers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "scrape proxies from free-proxy-list.net/anonymous-proxy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxies found: 398\n"
     ]
    }
   ],
   "source": [
    "def retrieve_proxies():\n",
    "    \n",
    "    proxies = set()\n",
    "\n",
    "    urls = ['https://free-proxy-list.net/anonymous-proxy.html', 'https://www.us-proxy.org/', \\\n",
    "            'https://www.sslproxies.org/', 'https://www.socks-proxy.net/']\n",
    "    \n",
    "    for url in urls:\n",
    "        res = requests.get(url, headers={'User-Agent' : ua.random})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print \"CONNECTION ERROR ON RETRIEVING PROXIES\"\n",
    "\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        elems = soup.findAll(\"table\", { \"id\" : \"proxylisttable\" })\n",
    "\n",
    "        for i in elems:\n",
    "            cells = i.find_all('td')\n",
    "            for cell in cells:\n",
    "                if cell.string != None and re.match('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', cell.string) != None: \n",
    "                    proxies.add(cell.string)\n",
    "                \n",
    "    return list(proxies)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        proxies = retrieve_proxies()\n",
    "        if len(proxies) > 0: break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "print \"proxies found: \" + str(len(proxies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step takes a list of asins as input and collects their review pages urls\n",
    "* I should check here if the product actually exists and mess with the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nproducts = ['B01H2E0J5M']\\n\\nwhile True:\\n    try:\\n        pages_hrefs = get_review_pages_hrefs(products)\\n        if len(pages_hrefs)>0: \\n            break\\n    except Exception as error:\\n        print('caught this error: ' + repr(error))\\n        time.sleep(0.5)\\n\\nprint pages_hrefs\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_review_pages_hrefs(products):\n",
    "    \n",
    "    pages_hrefs = set()\n",
    "    \n",
    "    for asin in products:\n",
    "        \n",
    "        url = 'https://www.amazon.com/gp/product/' + asin\n",
    "        \n",
    "        res = requests.get(url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                           headers={'User-Agent' : ua.random})\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print \"CONNECTION PROBLEM ON GETTING REVIEW PAGES HREFS\"\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        \n",
    "        if soup.title.text == \"Robot Check\": \n",
    "            print \"SGAMATO\"\n",
    "            continue\n",
    "\n",
    "        elems = soup.find(\"div\", { \"id\" : \"reviews-medley-footer\" })\n",
    "        elems2 = elems.find(\"a\", { \"class\" : \"a-link-emphasis\" })\n",
    "        \n",
    "        pages_hrefs.add(elems2.get(\"href\"))\n",
    "        \n",
    "        page_file = open(\"./pages/\" + asin + '.html', 'w+')\n",
    "        page_file.write(res.content)\n",
    "    \n",
    "    return pages_hrefs\n",
    "\"\"\"\n",
    "products = ['B01H2E0J5M']\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        pages_hrefs = get_review_pages_hrefs(products)\n",
    "        if len(pages_hrefs)>0: \n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print pages_hrefs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwhile True:\\n    try:\\n        max_page = find_max_pages(list(pages_hrefs)[0])\\n        if max_page>0: \\n            break\\n    except Exception as error:\\n        print('caught this error: ' + repr(error))\\n        time.sleep(0.5)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_pages(href):\n",
    "    hr_url = 'https://www.amazon.com/' + href\n",
    "    res = requests.get(hr_url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                       headers={'User-Agent' : ua.random})\n",
    "\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    if soup.title.text == \"Robot Check\": \n",
    "        print \"SGAMATO\"\n",
    "        \n",
    "    pag_ul = soup.find(\"ul\", {\"class\":\"a-pagination\"})\n",
    "\n",
    "    max_page = 0\n",
    "\n",
    "    for li in pag_ul.find_all(\"li\", {\"class\":\"page-button\"}):\n",
    "        max_page = li.a.text\n",
    "\n",
    "    return max_page\n",
    "\"\"\"\n",
    "while True:\n",
    "    try:\n",
    "        max_page = find_max_pages(list(pages_hrefs)[0])\n",
    "        if max_page>0: \n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print max_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now craft the url!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def craft_urls(base_url, max_pages):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    url_parts = base_url.strip().split(\"/\")\n",
    "    params = url_parts[-1].split('&')\n",
    "\n",
    "    for page_num in range(1, int(max_pages)):\n",
    "        chosen_params = ['ref=cm_cr_dp_d_show_all_btm_'+str(page_num)+'?ie=UTF8', 'pageNumber='+str(page_num)]\n",
    "\n",
    "        final_url = \"https://www.amazon.com/\"\n",
    "\n",
    "        for i, item in enumerate(url_parts):\n",
    "            if i < (len(url_parts)-1) and i>0: final_url += item + '/'\n",
    "\n",
    "        for param in chosen_params:\n",
    "            final_url += param + '&'\n",
    "\n",
    "        final_url = final_url[:-1]\n",
    "        \n",
    "        res.append(final_url)\n",
    "\n",
    "    return res\n",
    "    \n",
    "#final_urls_list = craft_urls(list(pages_hrefs)[0], max_page)\n",
    "#pprint(final_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_page(url, header, proxy, fails, results):\n",
    "    res = \"\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=20, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                           headers={'User-Agent' : ua.random})\n",
    "    except Exception as error:\n",
    "        print \"connection timed out\"\n",
    "        fails.put(url)\n",
    "    \n",
    "    if res == \"\":\n",
    "        print \"Something failed\"\n",
    "    elif res != \"\" and res.status_code != 200:\n",
    "        fails.put(url)\n",
    "    elif BeautifulSoup(res.content, 'html.parser').title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        fails.put(url)\n",
    "    else:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        review_boxes = soup.find_all(\"div\", {\"class\":\"review\"})\n",
    "\n",
    "        for box in review_boxes:\n",
    "            review = {}\n",
    "            review['title'] = box.find(\"a\", {\"class\":\"review-title\"}).text\n",
    "            review['text'] = box.find(\"span\", {\"class\":\"review-text\"}).text\n",
    "            review['date'] = box.find(\"span\", {\"class\":\"review-date\"}).text\n",
    "            review['rating'] = box.find(\"i\", {\"class\":\"review-rating\"}).span.text\n",
    "            review['author'] = box.find(\"a\", {\"class\":\"author\"}).text\n",
    "            review['author_id'] = box.find(\"a\", {\"class\":\"author\"}).get('href').split('/')[4]\n",
    "            results.put(review)\n",
    "\n",
    "def explore_pages(final_urls_list, max_page):\n",
    "    pages = []\n",
    "    threads = []\n",
    "    results = Queue.Queue()\n",
    "    fails = Queue.Queue()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, int(max_page)-1):\n",
    "\n",
    "        if i%10==0: \n",
    "            time.sleep(10)\n",
    "            print i,\n",
    "\n",
    "        f_url = final_urls_list[i]\n",
    "        proxy = proxies[i%len(proxies)]\n",
    "        t = threading.Thread(target=retrieve_page, args=(f_url, headers, proxy, fails, results))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        \n",
    "    return results, fails\n",
    "  \n",
    "#results, fails = explore_pages(final_urls_list, int(max_page))\n",
    "#print(\"elapsed time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nli = list(results.queue)\\nprint len(li)\\n\\nlif = list(fails.queue)\\nprint len(lif)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "li = list(results.queue)\n",
    "print len(li)\n",
    "\n",
    "lif = list(fails.queue)\n",
    "print len(lif)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresults_file = open(\"./results/\"+products[0]+\".json\", \\'w+\\')\\nexamined_file = open(\"examined_asins.txt\", \\'a\\')\\nfailed_file = open(\"failed_urls.txt\", \\'a\\')\\n\\nexamined_file.write(products[0])\\n\\nfor i in li:\\n    json.dump(i, results_file)\\n    results_file.write(\\'\\n\\')\\n    \\nfor i in lif:\\n    failed_file.write(i + \\'\\n\\')\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "results_file = open(\"./results/\"+products[0]+\".json\", 'w+')\n",
    "examined_file = open(\"examined_asins.txt\", 'a')\n",
    "failed_file = open(\"failed_urls.txt\", 'a')\n",
    "\n",
    "examined_file.write(products[0])\n",
    "\n",
    "for i in li:\n",
    "    json.dump(i, results_file)\n",
    "    results_file.write('\\n')\n",
    "    \n",
    "for i in lif:\n",
    "    failed_file.write(i + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_pages_questions = 0\\nwhile True:\\n    try: \\n        max_pages_questions = get_max_pages_questions()\\n        if max_pages_questions > 0:\\n            print \"question pages: \" + max_pages_questions\\n            break\\n    except Exception as error:\\n        print(\\'caught this error: \\' + repr(error))\\n        time.sleep(0.5)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_pages_questions():\n",
    "    url = \"https://www.amazon.com/ask/questions/asin/\" + 'B018IZ0VLQ'\n",
    "    res = requests.get(url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                       headers={'User-Agent' : ua.random})\n",
    "    \n",
    "    max_pages_questions = 0\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print \"CONNECTION ERROR ON RETRIEVING MAX NUMBER OF QUESTIONS\"\n",
    "    \n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    if soup.title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        \n",
    "    for item in soup.find(\"ul\", {\"class\":\"a-pagination\"}).find_all(\"li\", {\"class\":\"a-normal\"}):\n",
    "        max_pages_questions = item.text\n",
    "        \n",
    "    return max_pages_questions\n",
    "\"\"\"\n",
    "max_pages_questions = 0\n",
    "while True:\n",
    "    try: \n",
    "        max_pages_questions = get_max_pages_questions()\n",
    "        if max_pages_questions > 0:\n",
    "            print \"question pages: \" + max_pages_questions\n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_questions(url, header, proxy, fails, results):\n",
    "    res = \"\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=20, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, \\\n",
    "                           headers={'User-Agent' : ua.random})\n",
    "    except Exception as error:\n",
    "        print \"connection timed out\"\n",
    "        fails.put(url)\n",
    "    \n",
    "    if res == \"\":\n",
    "        print \"Something failed\"\n",
    "    elif res != \"\" and res.status_code != 200:\n",
    "        print \"response status: \" + str(res.status_code)\n",
    "        fails.put(url)\n",
    "    elif BeautifulSoup(res.content, 'html.parser').title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        fails.put(url)\n",
    "    else:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        question_boxes = soup.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})\n",
    "\n",
    "        for j, box in enumerate(question_boxes):\n",
    "            q_a_dict = {}\n",
    "            \n",
    "            if j==0: continue\n",
    "            for k, question in enumerate(box.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})):\n",
    "                if k==0: \n",
    "                    #print \"box: \" + str(j) + \" question: \" + question.a.text.strip()\n",
    "                    q_a_dict['question'] = question.a.text.strip()\n",
    "            for k, answer in enumerate(box.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})):\n",
    "                if k!=0: \n",
    "                    ranswer = \"\"\n",
    "                    if answer.find(\"span\", {\"class\":\"askLongText\"}):\n",
    "                        ranswer = answer.find(\"span\", {\"class\":\"askLongText\"}).text\n",
    "                        ranswer = ranswer.strip()[:-8]\n",
    "                    else: \n",
    "                        ranswer = answer.span.text\n",
    "                    q_a_dict['answer'] = ranswer.strip()\n",
    "                    #print \"box: \" + str(j) + \" answer: \" + ranswer\n",
    "            if 'answer' in q_a_dict and 'question' in q_a_dict: results.put(q_a_dict)\n",
    "\n",
    "def explore_questions(product, max_pages_questions):\n",
    "    pages = []\n",
    "    threads = []\n",
    "    results = Queue.Queue()\n",
    "    fails = Queue.Queue()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(1, int(max_pages_questions)):\n",
    "        \n",
    "        if i%10==0: \n",
    "            time.sleep(10)\n",
    "            print i,\n",
    "\n",
    "        f_url = \"https://www.amazon.com/ask/questions/asin/\" + product + '/' + str(i)\n",
    "        proxy = proxies[i%len(proxies)]\n",
    "        t = threading.Thread(target=retrieve_questions, args=(f_url, headers, proxy, fails, results))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        \n",
    "    return results, fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start(products):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for product in products:\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                proxies = retrieve_proxies()\n",
    "                if len(proxies) > 0: break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        print \"proxies found: \" + str(len(proxies))\n",
    "        \n",
    "        print \"examining product: \" + product\n",
    "        \n",
    "        pages_hrefs = []\n",
    "        \n",
    "        asins_with_problems = open(\"asins_with_problems.txt\", 'a')\n",
    "        \n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            try:\n",
    "                pages_hrefs = get_review_pages_hrefs([product])\n",
    "                if len(pages_hrefs)>0:\n",
    "                    break\n",
    "                else: print \"pages_hrefs not found\"\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "            if count%10==0: break\n",
    "        if count==10: \n",
    "            asins_with_problems.write(product + '\\n')\n",
    "            continue\n",
    "\n",
    "        print pages_hrefs\n",
    "\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            try:\n",
    "                max_page = find_max_pages(list(pages_hrefs)[0])\n",
    "                if max_page>0: \n",
    "                    break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "            if count%10==0: break\n",
    "        if count==10: \n",
    "            asins_with_problems.write(product + '\\n')\n",
    "            continue\n",
    "\n",
    "        print max_page\n",
    "\n",
    "        final_urls_list = craft_urls(list(pages_hrefs)[0], max_page)\n",
    "        \n",
    "        results, fails = explore_pages(final_urls_list, int(max_page))\n",
    "        \n",
    "        li = list(results.queue)\n",
    "        print \"\\n results found: \" + str(len(li))\n",
    "\n",
    "        lif = list(fails.queue)\n",
    "        print \"results failed: \" + str(len(lif))\n",
    "        \n",
    "        max_pages_questions = 0\n",
    "        while True:\n",
    "            try: \n",
    "                max_pages_questions = get_max_pages_questions()\n",
    "                if max_pages_questions > 0:\n",
    "                    print \"question pages: \" + max_pages_questions\n",
    "                    break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "                \n",
    "        results_questions, fails_questions = explore_questions(product, max_pages_questions)\n",
    "        \n",
    "        lir = list(results_questions.queue)\n",
    "        print \"\\n results found: \" + str(len(lir))\n",
    "        lifr = list(fails_questions.queue)\n",
    "        print \"results failed: \" + str(len(lifr))\n",
    "        \n",
    "        results_file = open(\"./results/\" + product + \".json\", 'w+')\n",
    "        questions_file = open(\"./questions/\" + product + \".json\", 'w+')\n",
    "        examined_file = open(\"examined_asins.txt\", 'a')\n",
    "        failed_file = open(\"failed_urls.txt\", 'a')\n",
    "        failed_questions_file = open(\"failed_questions_urls.txt\", 'a')\n",
    "\n",
    "        examined_file.write(product + '\\n')\n",
    "\n",
    "        for i in li:\n",
    "            json.dump(i, results_file)\n",
    "            results_file.write('\\n')\n",
    "\n",
    "        for i in lir:\n",
    "            json.dump(i, questions_file)\n",
    "            questions_file.write('\\n')\n",
    "            \n",
    "        for i in lif:\n",
    "            failed_file.write(i + '\\n')\n",
    "            \n",
    "        for i in lifr:\n",
    "            failed_questions_file.write(i + '\\n')\n",
    "    \n",
    "    print \"elapsed time: \" + str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 160 examined_products\n",
      "170 asins with problems\n",
      "34 products to examine\n",
      "proxies found: 387\n",
      "examining product: B01ANFTFV6\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 387\n",
      "examining product: B015ZKK2ES\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 387\n",
      "examining product: B00VWKKHKU\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/ZenFone-Unlocked-Cellphone-Silver-Warranty/product-reviews/B00VWKKHKU/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "254\n",
      "0 SGAMATO\n",
      " SGAMATO\n",
      " SGAMATOSGAMATO\n",
      " \n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO \n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "60 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "70 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "80 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "100 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "110 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "120 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "130 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "140 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "150 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO \n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "160 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "170 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "180 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "190 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "200 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "SGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "210 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "220 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "230 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "240 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "250 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "\n",
      " results found: 710\n",
      "results failed: 178\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "10 20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "60 SGAMATO\n",
      "70 80 90 SGAMATO\n",
      "SGAMATO \n",
      "SGAMATO\n",
      "\n",
      " results found: 524\n",
      "results failed: 12\n",
      "proxies found: 392\n",
      "examining product: B00VWKKHE6\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/ASUS-ZenFone-Unlocked-Cellphone-Warranty/product-reviews/B00VWKKHE6/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "254\n",
      "0 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "60 SGAMATO SGAMATO\n",
      "\n",
      "SGAMATO\n",
      " SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "70 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "80 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      " \n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "100 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATOSGAMATO\n",
      "SGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "110 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO \n",
      "SGAMATOSGAMATO \n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "120 SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "130 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "140 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "150 SGAMATO\n",
      " SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "160 SGAMATO\n",
      "SGAMATO SGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "170 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "180 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "190 SGAMATO\n",
      " SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "200 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "210 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "220 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "230 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "240 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "250 SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 677\n",
      "results failed: 181\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATOSGAMATO\n",
      "\n",
      "60 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "70 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "80 SGAMATO\n",
      "SGAMATO \n",
      "SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 450\n",
      "results failed: 26\n",
      "proxies found: 392\n",
      "examining product: B01IRTZUU0\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 392\n",
      "examining product: B0186S41RU\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 392\n",
      "examining product: B00I7EM752\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 392\n",
      "examining product: B01IB9H9MS\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 392\n",
      "examining product: B073XW4JHC\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/ZenFone-5-7-inch-storage-Unlocked-Warranty/product-reviews/B073XW4JHC/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "proxies found: 389\n",
      "examining product: B071XF4DTG\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 391\n",
      "examining product: B01LX7T4O3\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find'\",)\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B00TXR59J8\n",
      "set([u'/Asus-Padfone-Black-Phone-Smartphone/product-reviews/B00TXR59J8/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "3\n",
      "0 SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 0\n",
      "results failed: 2\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "20 SGAMATO\n",
      "30 SGAMATO\n",
      "SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "60 70 SGAMATO\n",
      "80 90 SGAMATO\n",
      "\n",
      " results found: 27\n",
      "results failed: 14\n",
      "proxies found: 389\n",
      "examining product: B014QR96G6\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B01LQV1YNS\n",
      "set([u'/ZS550KL-ZenFone-Unlocked-5-5-inch-Warranty/product-reviews/B01LQV1YNS/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "31\n",
      "0 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 70\n",
      "results failed: 23\n",
      "question pages: 100\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "20 30 SGAMATO\n",
      "40 50 connection timed out\n",
      "Something failed\n",
      "connection timed out\n",
      "Something failed\n",
      "60 70 SGAMATO\n",
      "SGAMATO\n",
      "80 90 \n",
      " results found: 247\n",
      "results failed: 9\n",
      "proxies found: 389\n",
      "examining product: B06X3Q5K3V\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B00L8UVLOA\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B01MZ46E4F\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find'\",)\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B00120LA4C\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B00TCVHGZU\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/ASUS-Smartwatch-Silver-Rose-Gold/product-reviews/B00TCVHGZU/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "28\n",
      "0 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      " \n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "\n",
      "SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "20 SGAMATOSGAMATO\n",
      " SGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 50\n",
      "results failed: 22\n",
      "question pages: 100\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "SGAMATO\n",
      "\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "40 SGAMATO \n",
      "SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "60 SGAMATO\n",
      "SGAMATO\n",
      "70 80 90 SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 124\n",
      "results failed: 22\n",
      "proxies found: 389\n",
      "examining product: B01I9JM23G\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B019DT8GMC\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 389\n",
      "examining product: B0186S41XE\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/Asus-Zenfone-Laser-ZE601KL-Smartphone/product-reviews/B0186S41XE/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n",
      "proxies found: 389\n",
      "examining product: B01B2MSEG6\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/Asus-ZenFone-ZE601KL-International-Warranty/product-reviews/B01B2MSEG6/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'])\n",
      "SGAMATO\n",
      "caught this error: AttributeError(\"'NoneType' object has no attribute 'find_all'\",)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c32f6fe01233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" products to examine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-407f523b3da2>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(products)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'caught this error: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "asins_to_examine = open(\"asins_to_examine.txt\", 'r')\n",
    "examined_asins = open(\"examined_asins.txt\", 'r')\n",
    "asins_with_problems = open(\"asins_with_problems.txt\", 'r')\n",
    "\n",
    "examined_asins_set = set()\n",
    "for line in examined_asins:\n",
    "    examined_asins_set.add(line.strip())\n",
    "    \n",
    "print str(len(examined_asins_set)) + \" examined_products\"\n",
    "\n",
    "asins_with_problems_set = set()\n",
    "for line in asins_with_problems:\n",
    "    asins_with_problems_set.add(line.strip())\n",
    "    \n",
    "print str(len(asins_with_problems_set)) + \" asins with problems\"\n",
    "\n",
    "products = []\n",
    "for line in asins_to_examine:\n",
    "    if line.strip() not in examined_asins_set and line.strip() not in asins_with_problems_set: \n",
    "        products.append(line.strip())\n",
    "    \n",
    "print str(len(products)) + \" products to examine\"\n",
    "start(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "amazon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
