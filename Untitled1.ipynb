{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- if SGAMATO, extend sleep time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import random\n",
    "import threading, Queue\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "header_list = ['Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "\t'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/601.2.7 (KHTML, like Gecko) Version/9.0.1 Safari/601.2.7',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11) AppleWebKit/601.1.56 (KHTML, like Gecko) Version/9.0 Safari/601.1.56',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:41.0) Gecko/20100101 Firefox/41.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.71 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.13) Gecko/20080311 Firefox/2.0.0.13',\n",
    "    'Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)']\n",
    "\n",
    "headers = [{'User-Agent' : i} for i in header_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "scrape proxies from free-proxy-list.net/anonymous-proxy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxies found: 100\n"
     ]
    }
   ],
   "source": [
    "def retrieve_proxies():\n",
    "    \n",
    "    proxies = []\n",
    "\n",
    "    url = 'https://free-proxy-list.net/anonymous-proxy.html'\n",
    "    res = requests.get(url, headers=headers[random.randint(0, len(headers)-1)])\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print \"CONNECTION ERROR ON RETRIEVING PROXIES\"\n",
    "    \n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "    elems = soup.findAll(\"table\", { \"id\" : \"proxylisttable\" })\n",
    "\n",
    "    for i in elems:\n",
    "        cells = i.find_all('td')\n",
    "        for cell in cells:\n",
    "            if cell.string != None and re.match('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', cell.string) != None: \n",
    "                proxies.append(cell.string)\n",
    "    \n",
    "    return proxies\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        proxies = retrieve_proxies()\n",
    "        if len(proxies) > 0: break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "print \"proxies found: \" + str(len(proxies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step takes a list of asins as input and collects their review pages urls\n",
    "* I should check here if the product actually exists and mess with the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nproducts = ['B01H2E0J5M']\\n\\nwhile True:\\n    try:\\n        pages_hrefs = get_review_pages_hrefs(products)\\n        if len(pages_hrefs)>0: \\n            break\\n    except Exception as error:\\n        print('caught this error: ' + repr(error))\\n        time.sleep(0.5)\\n\\nprint pages_hrefs\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_review_pages_hrefs(products):\n",
    "    \n",
    "    pages_hrefs = set()\n",
    "    \n",
    "    for asin in products:\n",
    "        \n",
    "        url = 'https://www.amazon.com/gp/product/' + asin\n",
    "        \n",
    "        res = requests.get(url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, headers=headers[random.randint(0, len(headers)-1)])\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print \"CONNECTION PROBLEM ON GETTING REVIEW PAGES HREFS\"\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        \n",
    "        if soup.title.text == \"Robot Check\": \n",
    "            print \"SGAMATO\"\n",
    "            continue\n",
    "\n",
    "        elems = soup.find(\"div\", { \"id\" : \"reviews-medley-footer\" })\n",
    "        elems2 = elems.find(\"a\", { \"class\" : \"a-link-emphasis\" })\n",
    "        \n",
    "        pages_hrefs.add(elems2.get(\"href\"))\n",
    "        \n",
    "        page_file = open(\"./pages/\" + asin + '.html', 'w+')\n",
    "        page_file.write(res.content)\n",
    "    \n",
    "    return pages_hrefs\n",
    "\"\"\"\n",
    "products = ['B01H2E0J5M']\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        pages_hrefs = get_review_pages_hrefs(products)\n",
    "        if len(pages_hrefs)>0: \n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print pages_hrefs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwhile True:\\n    try:\\n        max_page = find_max_pages(list(pages_hrefs)[0])\\n        if max_page>0: \\n            break\\n    except Exception as error:\\n        print('caught this error: ' + repr(error))\\n        time.sleep(0.5)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_pages(href):\n",
    "    hr_url = 'https://www.amazon.com/' + href\n",
    "    res = requests.get(hr_url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, headers=headers[random.randint(0, len(headers)-1)])\n",
    "\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    if soup.title.text == \"Robot Check\": \n",
    "        print \"SGAMATO\"\n",
    "        \n",
    "    pag_ul = soup.find(\"ul\", {\"class\":\"a-pagination\"})\n",
    "\n",
    "    max_page = 0\n",
    "\n",
    "    for li in pag_ul.find_all(\"li\", {\"class\":\"page-button\"}):\n",
    "        max_page = li.a.text\n",
    "\n",
    "    return max_page\n",
    "\"\"\"\n",
    "while True:\n",
    "    try:\n",
    "        max_page = find_max_pages(list(pages_hrefs)[0])\n",
    "        if max_page>0: \n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print max_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now craft the url!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def craft_urls(base_url, max_pages):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    url_parts = base_url.strip().split(\"/\")\n",
    "    params = url_parts[-1].split('&')\n",
    "\n",
    "    for page_num in range(1, int(max_pages)):\n",
    "        chosen_params = ['ref=cm_cr_dp_d_show_all_btm_'+str(page_num)+'?ie=UTF8', 'pageNumber='+str(page_num)]\n",
    "\n",
    "        final_url = \"https://www.amazon.com/\"\n",
    "\n",
    "        for i, item in enumerate(url_parts):\n",
    "            if i < (len(url_parts)-1) and i>0: final_url += item + '/'\n",
    "\n",
    "        for param in chosen_params:\n",
    "            final_url += param + '&'\n",
    "\n",
    "        final_url = final_url[:-1]\n",
    "        \n",
    "        res.append(final_url)\n",
    "\n",
    "    return res\n",
    "    \n",
    "#final_urls_list = craft_urls(list(pages_hrefs)[0], max_page)\n",
    "#pprint(final_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_page(url, header, proxy, fails, results):\n",
    "    res = \"\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=20, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, headers=headers[random.randint(0, len(headers)-1)])\n",
    "    except Exception as error:\n",
    "        print \"connection timed out\"\n",
    "        fails.put(url)\n",
    "    \n",
    "    if res == \"\":\n",
    "        print \"Something failed\"\n",
    "    elif res != \"\" and res.status_code != 200:\n",
    "        fails.put(url)\n",
    "    elif BeautifulSoup(res.content, 'html.parser').title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        fails.put(url)\n",
    "    else:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        review_boxes = soup.find_all(\"div\", {\"class\":\"review\"})\n",
    "\n",
    "        for box in review_boxes:\n",
    "            review = {}\n",
    "            review['title'] = box.find(\"a\", {\"class\":\"review-title\"}).text\n",
    "            review['text'] = box.find(\"span\", {\"class\":\"review-text\"}).text\n",
    "            review['date'] = box.find(\"span\", {\"class\":\"review-date\"}).text\n",
    "            review['rating'] = box.find(\"i\", {\"class\":\"review-rating\"}).span.text\n",
    "            review['author'] = box.find(\"a\", {\"class\":\"author\"}).text\n",
    "            review['author_id'] = box.find(\"a\", {\"class\":\"author\"}).get('href').split('/')[4]\n",
    "            results.put(review)\n",
    "\n",
    "def explore_pages(final_urls_list, max_page):\n",
    "    pages = []\n",
    "    threads = []\n",
    "    results = Queue.Queue()\n",
    "    fails = Queue.Queue()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, int(max_page)-1):\n",
    "\n",
    "        if i%10==0: \n",
    "            time.sleep(10)\n",
    "            print i,\n",
    "\n",
    "        f_url = final_urls_list[i]\n",
    "        proxy = proxies[i%len(proxies)]\n",
    "        t = threading.Thread(target=retrieve_page, args=(f_url, headers, proxy, fails, results))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        \n",
    "    return results, fails\n",
    "  \n",
    "#results, fails = explore_pages(final_urls_list, int(max_page))\n",
    "#print(\"elapsed time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nli = list(results.queue)\\nprint len(li)\\n\\nlif = list(fails.queue)\\nprint len(lif)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "li = list(results.queue)\n",
    "print len(li)\n",
    "\n",
    "lif = list(fails.queue)\n",
    "print len(lif)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresults_file = open(\"./results/\"+products[0]+\".json\", \\'w+\\')\\nexamined_file = open(\"examined_asins.txt\", \\'a\\')\\nfailed_file = open(\"failed_urls.txt\", \\'a\\')\\n\\nexamined_file.write(products[0])\\n\\nfor i in li:\\n    json.dump(i, results_file)\\n    results_file.write(\\'\\n\\')\\n    \\nfor i in lif:\\n    failed_file.write(i + \\'\\n\\')\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "results_file = open(\"./results/\"+products[0]+\".json\", 'w+')\n",
    "examined_file = open(\"examined_asins.txt\", 'a')\n",
    "failed_file = open(\"failed_urls.txt\", 'a')\n",
    "\n",
    "examined_file.write(products[0])\n",
    "\n",
    "for i in li:\n",
    "    json.dump(i, results_file)\n",
    "    results_file.write('\\n')\n",
    "    \n",
    "for i in lif:\n",
    "    failed_file.write(i + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_pages_questions = 0\\nwhile True:\\n    try: \\n        max_pages_questions = get_max_pages_questions()\\n        if max_pages_questions > 0:\\n            print \"question pages: \" + max_pages_questions\\n            break\\n    except Exception as error:\\n        print(\\'caught this error: \\' + repr(error))\\n        time.sleep(0.5)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_pages_questions():\n",
    "    url = \"https://www.amazon.com/ask/questions/asin/\" + 'B018IZ0VLQ'\n",
    "    res = requests.get(url, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, headers=headers[random.randint(0, len(headers)-1)])\n",
    "    \n",
    "    max_pages_questions = 0\n",
    "    \n",
    "    if res.status_code != 200:\n",
    "        print \"CONNECTION ERROR ON RETRIEVING MAX NUMBER OF QUESTIONS\"\n",
    "    \n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    if soup.title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        \n",
    "    for item in soup.find(\"ul\", {\"class\":\"a-pagination\"}).find_all(\"li\", {\"class\":\"a-normal\"}):\n",
    "        max_pages_questions = item.text\n",
    "        \n",
    "    return max_pages_questions\n",
    "\"\"\"\n",
    "max_pages_questions = 0\n",
    "while True:\n",
    "    try: \n",
    "        max_pages_questions = get_max_pages_questions()\n",
    "        if max_pages_questions > 0:\n",
    "            print \"question pages: \" + max_pages_questions\n",
    "            break\n",
    "    except Exception as error:\n",
    "        print('caught this error: ' + repr(error))\n",
    "        time.sleep(0.5)\n",
    "\"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_questions(url, header, proxy, fails, results):\n",
    "    res = \"\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=20, proxies={\"http\":proxies[random.randint(0, len(proxies)-1)]}, headers=headers[random.randint(0, len(headers)-1)])\n",
    "    except Exception as error:\n",
    "        print \"connection timed out\"\n",
    "        fails.put(url)\n",
    "    \n",
    "    if res == \"\":\n",
    "        print \"Something failed\"\n",
    "    elif res != \"\" and res.status_code != 200:\n",
    "        print \"response status: \" + str(res.status_code)\n",
    "        fails.put(url)\n",
    "    elif BeautifulSoup(res.content, 'html.parser').title.text == \"Robot Check\":\n",
    "        print \"SGAMATO\"\n",
    "        fails.put(url)\n",
    "    else:\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        question_boxes = soup.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})\n",
    "\n",
    "        for j, box in enumerate(question_boxes):\n",
    "            q_a_dict = {}\n",
    "            \n",
    "            if j==0: continue\n",
    "            for k, question in enumerate(box.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})):\n",
    "                if k==0: \n",
    "                    #print \"box: \" + str(j) + \" question: \" + question.a.text.strip()\n",
    "                    q_a_dict['question'] = question.a.text.strip()\n",
    "            for k, answer in enumerate(box.find_all(\"div\", {\"class\":\"a-fixed-left-grid-col a-col-right\"})):\n",
    "                if k!=0: \n",
    "                    ranswer = \"\"\n",
    "                    if answer.find(\"span\", {\"class\":\"askLongText\"}):\n",
    "                        ranswer = answer.find(\"span\", {\"class\":\"askLongText\"}).text\n",
    "                        ranswer = ranswer.strip()[:-8]\n",
    "                    else: \n",
    "                        ranswer = answer.span.text\n",
    "                    q_a_dict['answer'] = ranswer.strip()\n",
    "                    #print \"box: \" + str(j) + \" answer: \" + ranswer\n",
    "            if 'answer' in q_a_dict and 'question' in q_a_dict: results.put(q_a_dict)\n",
    "\n",
    "def explore_questions(product, max_pages_questions):\n",
    "    pages = []\n",
    "    threads = []\n",
    "    results = Queue.Queue()\n",
    "    fails = Queue.Queue()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(1, int(max_pages_questions)):\n",
    "        \n",
    "        if i%10==0: \n",
    "            time.sleep(10)\n",
    "            print i,\n",
    "\n",
    "        f_url = \"https://www.amazon.com/ask/questions/asin/\" + product + '/' + str(i)\n",
    "        proxy = proxies[i%len(proxies)]\n",
    "        t = threading.Thread(target=retrieve_questions, args=(f_url, headers, proxy, fails, results))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        \n",
    "    return results, fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start(products):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for product in products:\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                proxies = retrieve_proxies()\n",
    "                if len(proxies) > 0: break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        print \"proxies found: \" + str(len(proxies))\n",
    "        \n",
    "        print \"examining product: \" + product\n",
    "        \n",
    "        pages_hrefs = []\n",
    "        \n",
    "        asins_with_problems = open(\"asins_with_problems.txt\", 'a')\n",
    "        \n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            try:\n",
    "                pages_hrefs = get_review_pages_hrefs([product])\n",
    "                if len(pages_hrefs)>0:\n",
    "                    break\n",
    "                else: print \"pages_hrefs not found\"\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "            if count%10==0: break\n",
    "        if count==10: \n",
    "            asins_with_problems.write(product + '\\n')\n",
    "            continue\n",
    "\n",
    "        print pages_hrefs\n",
    "\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            try:\n",
    "                max_page = find_max_pages(list(pages_hrefs)[0])\n",
    "                if max_page>0: \n",
    "                    break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "            if count%10==0: break\n",
    "        if count==10: \n",
    "            asins_with_problems.write(product + '\\n')\n",
    "            continue\n",
    "\n",
    "        print max_page\n",
    "\n",
    "        final_urls_list = craft_urls(list(pages_hrefs)[0], max_page)\n",
    "        \n",
    "        results, fails = explore_pages(final_urls_list, int(max_page))\n",
    "        \n",
    "        li = list(results.queue)\n",
    "        print \"\\n results found: \" + str(len(li))\n",
    "\n",
    "        lif = list(fails.queue)\n",
    "        print \"results failed: \" + str(len(lif))\n",
    "        \n",
    "        max_pages_questions = 0\n",
    "        while True:\n",
    "            try: \n",
    "                max_pages_questions = get_max_pages_questions()\n",
    "                if max_pages_questions > 0:\n",
    "                    print \"question pages: \" + max_pages_questions\n",
    "                    break\n",
    "            except Exception as error:\n",
    "                print('caught this error: ' + repr(error))\n",
    "                time.sleep(1)\n",
    "                \n",
    "        results_questions, fails_questions = explore_questions(product, max_pages_questions)\n",
    "        \n",
    "        lir = list(results_questions.queue)\n",
    "        print \"\\n results found: \" + str(len(lir))\n",
    "        lifr = list(fails_questions.queue)\n",
    "        print \"results failed: \" + str(len(lifr))\n",
    "        \n",
    "        results_file = open(\"./results/\" + product + \".json\", 'w+')\n",
    "        questions_file = open(\"./questions/\" + product + \".json\", 'w+')\n",
    "        examined_file = open(\"examined_asins.txt\", 'a')\n",
    "        failed_file = open(\"failed_urls.txt\", 'a')\n",
    "        failed_questions_file = open(\"failed_questions_urls.txt\", 'a')\n",
    "\n",
    "        examined_file.write(product + '\\n')\n",
    "\n",
    "        for i in li:\n",
    "            json.dump(i, results_file)\n",
    "            results_file.write('\\n')\n",
    "\n",
    "        for i in lir:\n",
    "            json.dump(i, questions_file)\n",
    "            questions_file.write('\\n')\n",
    "            \n",
    "        for i in lif:\n",
    "            failed_file.write(i + '\\n')\n",
    "            \n",
    "        for i in lifr:\n",
    "            failed_questions_file.write(i + '\\n')\n",
    "    \n",
    "    print \"elapsed time: \" + str(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 examined_products\n",
      "83 asins with problems\n",
      "198 products to examine\n",
      "proxies found: 100\n",
      "examining product: B00LGQ9YUO\n",
      "set([u'/Original-OnePlus-White-Black-Android/product-reviews/B00LGQ9YUO/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=avp_only_reviews'])\n",
      "2\n",
      "0 \n",
      " results found: 10\n",
      "results failed: 0\n",
      "question pages: 100\n",
      "10 20 30 40 50 60 70 80 90 \n",
      " results found: 24\n",
      "results failed: 0\n",
      "proxies found: 100\n",
      "examining product: B00YD547Q6\n",
      "set([u'/Apple-iPhone-16GB-Unlocked-Smartphone/product-reviews/B00YD547Q6/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=avp_only_reviews'])\n",
      "230\n",
      "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 \n",
      " results found: 2290\n",
      "results failed: 0\n",
      "question pages: 100\n",
      "10 20 30 40 50 60 70 80 90 \n",
      " results found: 496\n",
      "results failed: 0\n",
      "proxies found: 100\n",
      "examining product: B00YD54HZ2\n",
      "set([u'/Apple-iPhone-Plus-Unlocked-Smartphone/product-reviews/B00YD54HZ2/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=avp_only_reviews'])\n",
      "22\n",
      "0 10 20 \n",
      " results found: 210\n",
      "results failed: 0\n",
      "question pages: 100\n",
      "10 20 30 40 50 60 70 80 90 \n",
      " results found: 115\n",
      "results failed: 0\n",
      "proxies found: 100\n",
      "examining product: B01N9YOF3R\n",
      "set([u'/Apple-iPhone-Factory-Unlocked-Smartphone/product-reviews/B01N9YOF3R/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=avp_only_reviews'])\n",
      "3\n",
      "0 \n",
      " results found: 20\n",
      "results failed: 0\n",
      "question pages: 100\n",
      "10 20 30 40 50 60 70 80 90 \n",
      " results found: 48\n",
      "results failed: 0\n",
      "proxies found: 100\n",
      "examining product: B00NQGP42Y\n",
      "set([u'/Apple-Factory-Unlocked-Internal-Smartphone/product-reviews/B00NQGP42Y/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=avp_only_reviews'])\n",
      "317\n",
      "0 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "60 SGAMATOSGAMATO\n",
      " SGAMATO\n",
      "\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "70 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATO\n",
      "SGAMATO\n",
      "\n",
      "SGAMATO\n",
      "80 SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "100 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "110 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "120 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "130 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "140 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "150 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "160 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "170 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "180 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "190 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO \n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "200 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO \n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "210 SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "220 SGAMATO\n",
      " SGAMATO SGAMATOSGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "\n",
      "\n",
      "230 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "240 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATO\n",
      "\n",
      "250 SGAMATO\n",
      "SGAMATO SGAMATO\n",
      "\n",
      "SGAMATOSGAMATOSGAMATO\n",
      "\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "260 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "270 SGAMATO \n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "280 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "290 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      " SGAMATO\n",
      "\n",
      "300 SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      " SGAMATOSGAMATO\n",
      "\n",
      " SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "310 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "\n",
      " results found: 1380\n",
      "results failed: 178\n",
      "question pages: 100\n",
      "10 20 SGAMATO\n",
      "30 40 50 60 70 80 90 \n",
      " results found: 665\n",
      "results failed: 1\n",
      "CONNECTION ERROR ON RETRIEVING PROXIES\n",
      "proxies found: 100\n",
      "examining product: B00YD545CC\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/Apple-Unlocked-Smartphone-Certified-Refurbished/product-reviews/B00YD545CC/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=avp_only_reviews'])\n",
      "36\n",
      "0 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO SGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 \n",
      " results found: 170\n",
      "results failed: 18\n",
      "question pages: 100\n",
      "10 20 30 40 50 60 70 80 90 \n",
      " results found: 220\n",
      "results failed: 0\n",
      "proxies found: 100\n",
      "examining product: B0097BUH1U\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "proxies found: 100\n",
      "examining product: B00YD548Q0\n",
      "SGAMATO\n",
      "pages_hrefs not found\n",
      "set([u'/Apple-iPhone-64GB-Unlocked-Smartphone/product-reviews/B00YD548Q0/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=avp_only_reviews'])\n",
      "230\n",
      "0 SGAMATO\n",
      "SGAMATO\n",
      " SGAMATOSGAMATO\n",
      "SGAMATO\n",
      "\n",
      "SGAMATO\n",
      "10 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      " \n",
      "SGAMATO\n",
      "SGAMATO\n",
      "20 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "30 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      " SGAMATO\n",
      "40 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "50 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "60 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "70 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "80 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "90 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "100 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "110 SGAMATOSGAMATO\n",
      "\n",
      "SGAMATOSGAMATO\n",
      "\n",
      "  SGAMATOSGAMATO\n",
      "\n",
      "SGAMATO\n",
      "120 SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n",
      "SGAMATO\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c32f6fe01233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" products to examine\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-407f523b3da2>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(products)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mfinal_urls_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcraft_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages_hrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplore_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_urls_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d24faace9af2>\u001b[0m in \u001b[0;36mexplore_pages\u001b[0;34m(final_urls_list, max_page)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "asins_to_examine = open(\"asins_to_examine.txt\", 'r')\n",
    "examined_asins = open(\"examined_asins.txt\", 'r')\n",
    "asins_with_problems = open(\"asins_with_problems.txt\", 'r')\n",
    "\n",
    "examined_asins_set = set()\n",
    "for line in examined_asins:\n",
    "    examined_asins_set.add(line.strip())\n",
    "    \n",
    "print str(len(examined_asins_set)) + \" examined_products\"\n",
    "\n",
    "asins_with_problems_set = set()\n",
    "for line in asins_with_problems:\n",
    "    asins_with_problems_set.add(line.strip())\n",
    "    \n",
    "print str(len(asins_with_problems_set)) + \" asins with problems\"\n",
    "\n",
    "products = []\n",
    "for line in asins_to_examine:\n",
    "    if line.strip() not in examined_asins_set and line.strip() not in asins_with_problems_set: \n",
    "        products.append(line.strip())\n",
    "    \n",
    "print str(len(products)) + \" products to examine\"\n",
    "start(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "amazon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
